{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import socket\n",
    "import wave\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Conv1D, AvgPool1D, MaxPool1D, ZeroPadding1D, BatchNormalization, Flatten, Dense, Activation\n",
    "from keras.activations import softmax\n",
    "from keras.utils import get_file\n",
    "from keras.utils import to_categorical\n",
    "import xenocanto\n",
    "import random\n",
    "import os\n",
    "import librosa\n",
    "import soundfile\n",
    "from pydub import AudioSegment "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download, cache and extract birds data from Xeno-Canto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "birds = ['Passer domesticus','Emberiza calandra','Fringilla coelebs']\n",
    "dataset_dir = Path('dataset')\n",
    "CLASSES = []\n",
    "if not (dataset_dir).exists(): # Assume dataset already downloaded/extracted if directory is present\n",
    "    for bird in birds : \n",
    "        xenocanto.metadata([bird,\"type:song\",\"q:A\"])\n",
    "        xenocanto.download([bird,\"type:song\",\"q:B\"], 2)\n",
    "        await xenocanto.download([bird,\"type:song\",\"q:A\"], 2)\n",
    "        await xenocanto.download([bird,\"type:song\",\"q:B\"], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Remove silent parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not (dataset_dir/'testing_list.txt').exists():\n",
    "    CLASSES = [c for c in os.listdir(dataset_dir/\"audio\") if os.path.isdir(dataset_dir/\"audio\"/c)]\n",
    "    \n",
    "    for c in CLASSES:\n",
    "        files = [f for f in os.listdir(dataset_dir/\"audio\"/c) if f.endswith('.mp3')]\n",
    "        for file in files:\n",
    "            try:\n",
    "                waveform, sample_rate = librosa.load(dataset_dir/\"audio\"/c/str(file))\n",
    "                print(dataset_dir/\"audio\"/c/str(file))\n",
    "                waveform = librosa.effects.trim(waveform, top_db=10)[0]\n",
    "                soundfile.write(\"dataset/audio/\"+c+\"/\"+str(file), waveform, sample_rate)\n",
    "                \n",
    "            except Exception as e:\n",
    "                os.remove(str(dataset_dir)+\"/audio/\"+c+\"/\"+str(file))\n",
    "                print(f\"Error processing {file}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES = [c for c in os.listdir(dataset_dir/\"audio\") if os.path.isdir(dataset_dir/\"audio\"/c)]\n",
    "\n",
    "if not (dataset_dir/'testing_list.txt').exists():\n",
    "    \n",
    "    for c in CLASSES:\n",
    "        files = [f for f in os.listdir(dataset_dir/\"audio\"/c) if f.endswith('.mp3')]\n",
    "        for file in files:\n",
    "            try:\n",
    "                sound = AudioSegment.from_mp3(dataset_dir/\"audio\"/c/str(file))\n",
    "                sound.export(dataset_dir/\"audio\"/c/str(file).replace('.mp3','.wav'), format=\"wav\")\n",
    "                os.remove(dataset_dir/\"audio\"/c/str(file))\n",
    "            except Exception as e:\n",
    "                os.remove(dataset_dir/\"audio\"/c/str(file))\n",
    "                print(f\"Error processing {file}: {e}\")\n",
    "    \n",
    "num_rec = min(len(os.listdir(os.path.join(dataset_dir, \"audio\", c))) for c in CLASSES)\n",
    "num_test = int(num_rec * 0.1)\n",
    "\n",
    "for c in CLASSES :\n",
    "    files = os.listdir(dataset_dir/\"audio\"/c)\n",
    "    for f in files[num_rec:] :\n",
    "        os.remove(dataset_dir/\"audio\"/c/f)\n",
    "os.open(dataset_dir/'testing_list.txt', os.O_CREAT)\n",
    "os.open(dataset_dir/'validation_list.txt', os.O_CREAT)\n",
    "for c in CLASSES :\n",
    "    recs = [ rec for rec in os.listdir(dataset_dir/'audio'/c) if rec.endswith('.wav') ]\n",
    "    randomrecs = random.sample(recs, num_test*2)\n",
    "    for rec in randomrecs:\n",
    "        if randomrecs.index(rec) < num_test:\n",
    "                with open(dataset_dir/'testing_list.txt', 'a') as f:\n",
    "                    f.write(c + '/' + rec +'\\n')\n",
    "        else : \n",
    "            with open(dataset_dir/'validation_list.txt', 'a') as f:\n",
    "                    f.write(c + '/' + rec +'\\n')\n",
    "\n",
    "with (dataset_dir/'testing_list.txt').open() as f:\n",
    "    testing_list = f.read().splitlines()\n",
    "    \n",
    "x_train = []\n",
    "y_train = []\n",
    "x_test = []\n",
    "y_test = []\n",
    "audiopath = dataset_dir/'audio'\n",
    "\n",
    "for recording in audiopath.glob(f'**/*.wav'):\n",
    "    if recording.parent.name not in CLASSES:\n",
    "        continue\n",
    "    \n",
    "    label = CLASSES.index(recording.parent.name)\n",
    "    with wave.open(str(recording)) as f:\n",
    "        data = np.frombuffer(f.readframes(f.getnframes()), dtype=np.int16).copy()\n",
    "    \n",
    "    data = data.astype(np.float32)\n",
    "    data.resize((16000, 1))\n",
    "    \n",
    "    if str(recording.relative_to(audiopath)).replace('\\\\','/') in testing_list:\n",
    "        x_test.append(data)\n",
    "        y_test.append(label)\n",
    "    else:\n",
    "        x_train.append(data)\n",
    "        y_train.append(label)\n",
    "\n",
    "x_train = np.array(x_train)\n",
    "y_train = to_categorical(np.array(y_train))\n",
    "x_test = np.array(x_test)\n",
    "y_test = to_categorical(np.array(y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1293 training samples and 231 testing samples\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "if not (dataset_dir/'testing_list.txt').exists(): # Assume dataset already downloaded/extracted if testing list is present\n",
    "\n",
    "    CLASSES = os.listdir(dataset_dir/\"audio\")\n",
    "    \n",
    "    for c in CLASSES :\n",
    "        files = os.listdir(dataset_dir/\"audio\"/c)\n",
    "        for file in files:\n",
    "            if file.endswith('.wav'):\n",
    "                try:\n",
    "                    wave.open(str(dataset_dir)+\"/audio/\"+c+\"/\"+str(file)) \n",
    "                except:\n",
    "                    os.remove(str(dataset_dir)+\"/audio/\"+c+\"/\"+str(file)) \n",
    "                \n",
    "    numOfrec =min([len(os.listdir(dataset_dir/\"audio\"/c)) for c in CLASSES])\n",
    "    num_test = int(numOfrec*0.1)\n",
    "    for c in CLASSES :\n",
    "        files = os.listdir(dataset_dir/\"audio\"/c)\n",
    "        for f in files[numOfrec:] :\n",
    "            os.remove(dataset_dir/\"audio\"/c/f)\n",
    "    os.open(dataset_dir/'testing_list.txt', os.O_CREAT)\n",
    "    os.open(dataset_dir/'validation_list.txt', os.O_CREAT)\n",
    "    for c in CLASSES :\n",
    "        recs = [ rec for rec in os.listdir(dataset_dir/'audio'/c) if rec.endswith('.wav') ]\n",
    "        randomrecs = random.sample(recs, num_test*2)\n",
    "        for rec in randomrecs:\n",
    "            if randomrecs.index(rec) < num_test:\n",
    "                 with open(dataset_dir/'testing_list.txt', 'a') as f:\n",
    "                        f.write(c + '/' + rec +'\\n')\n",
    "            else : \n",
    "                with open(dataset_dir/'validation_list.txt', 'a') as f:\n",
    "                        f.write(c + '/' + rec +'\\n')\n",
    "# Classes to handle, ordered by label\n",
    "with (dataset_dir/'testing_list.txt').open() as f:\n",
    "    testing_list = f.read().splitlines()\n",
    "CLASSES = os.listdir(dataset_dir/\"audio\")\n",
    "x_train = []\n",
    "y_train = []\n",
    "x_test = []\n",
    "y_test = []\n",
    "audiopath = dataset_dir/'audio'\n",
    "for recording in audiopath.glob(f'**/*.wav'):\n",
    "    if not recording.parent.name in CLASSES: # Ignore unused classes\n",
    "        continue\n",
    "    \n",
    "    label = CLASSES.index(recording.parent.name) # Assign class number\n",
    "    with wave.open(str(recording)) as f: # Read wave file\n",
    "        data = np.frombuffer(f.readframes(f.getnframes()), dtype=np.int16).copy() # As 16-bit signed integer\n",
    "\n",
    "    data = data.astype(np.float32) # Convert to 32-bit floating-point\n",
    "    data.resize((16000, 1)) # Resize to 2s (10kHz) with zero-padding, 1 channel\n",
    "    if str(recording.relative_to(audiopath)).replace('\\\\','/') in testing_list: # Assign to test set if file in test list\n",
    "        x_test.append(data)\n",
    "        y_test.append(label)\n",
    "    else:\n",
    "        x_train.append(data)\n",
    "        y_train.append(label)\n",
    "\n",
    "print(f'Loaded {len(x_train)} training samples and {len(x_test)} testing samples')\n",
    "x_train = np.array(x_train)\n",
    "y_train = to_categorical(np.array(y_train))\n",
    "x_test = np.array(x_test)\n",
    "y_test = to_categorical(np.array(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare for inference with fixed-point Q7.9 samples by scaling input data accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIXED_POINT = 9\n",
    "x_train /= 2**FIXED_POINT\n",
    "x_test  /= 2**FIXED_POINT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export small dataset (250 random vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "perms = np.random.permutation(len(y_test))[0:250]\n",
    "x_test_250 = x_test[perms]\n",
    "y_test_250 = y_test[perms]\n",
    "np.savetxt('x_test_gsc_250.csv', x_test_250.reshape((x_test_250.shape[0], -1)), delimiter=',', fmt='%s')\n",
    "np.savetxt('y_test_gsc_250.csv', y_test_250, delimiter=',', fmt='%s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Build model M5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d (Conv1D)             (None, 1599, 8)           168       \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1  (None, 799, 8)            0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 198, 16)           1040      \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPoolin  (None, 99, 16)            0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 48, 32)            2080      \n",
      "                                                                 \n",
      " max_pooling1d_2 (MaxPoolin  (None, 24, 32)            0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " conv1d_3 (Conv1D)           (None, 23, 64)            4160      \n",
      "                                                                 \n",
      " average_pooling1d (Average  (None, 5, 64)             0         \n",
      " Pooling1D)                                                      \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 320)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 3)                 963       \n",
      "                                                                 \n",
      " activation (Activation)     (None, 3)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8411 (32.86 KB)\n",
      "Trainable params: 8411 (32.86 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Input(shape=(16000, 1)))\n",
    "model.add(Conv1D(filters=8, kernel_size=20, strides=10,activation='relu'))\n",
    "model.add(MaxPool1D(pool_size=2))\n",
    "model.add(Conv1D(filters=16, kernel_size=8, strides=4, activation='relu'))\n",
    "model.add(MaxPool1D(pool_size=2))\n",
    "model.add(Conv1D(filters=32, kernel_size=4, strides=2, activation='relu'))\n",
    "model.add(MaxPool1D(pool_size=2))\n",
    "model.add(Conv1D(filters=64, kernel_size=2, activation='relu'))\n",
    "model.add(AvgPool1D(4))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=3))\n",
    "model.add(Activation('softmax')) \n",
    "opt = tf.keras.optimizers.Adam(learning_rate=10e-3)\n",
    "\n",
    "model.summary()\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "4/4 [==============================] - 2s 171ms/step - loss: 1.5962 - categorical_accuracy: 0.3256 - val_loss: 0.9731 - val_categorical_accuracy: 0.5714\n",
      "Epoch 2/20\n",
      "4/4 [==============================] - 0s 89ms/step - loss: 0.9573 - categorical_accuracy: 0.5391 - val_loss: 0.8797 - val_categorical_accuracy: 0.6494\n",
      "Epoch 3/20\n",
      "4/4 [==============================] - 0s 86ms/step - loss: 0.8967 - categorical_accuracy: 0.6110 - val_loss: 0.8553 - val_categorical_accuracy: 0.6147\n",
      "Epoch 4/20\n",
      "4/4 [==============================] - 0s 77ms/step - loss: 0.8266 - categorical_accuracy: 0.6582 - val_loss: 0.7022 - val_categorical_accuracy: 0.7403\n",
      "Epoch 5/20\n",
      "4/4 [==============================] - 0s 80ms/step - loss: 0.6647 - categorical_accuracy: 0.7757 - val_loss: 0.5360 - val_categorical_accuracy: 0.7879\n",
      "Epoch 6/20\n",
      "4/4 [==============================] - 0s 79ms/step - loss: 0.5602 - categorical_accuracy: 0.7966 - val_loss: 0.4711 - val_categorical_accuracy: 0.8312\n",
      "Epoch 7/20\n",
      "4/4 [==============================] - 0s 71ms/step - loss: 0.5197 - categorical_accuracy: 0.8252 - val_loss: 0.4612 - val_categorical_accuracy: 0.8312\n",
      "Epoch 8/20\n",
      "4/4 [==============================] - 0s 71ms/step - loss: 0.4753 - categorical_accuracy: 0.8183 - val_loss: 0.5370 - val_categorical_accuracy: 0.8139\n",
      "Epoch 9/20\n",
      "4/4 [==============================] - 0s 73ms/step - loss: 0.4746 - categorical_accuracy: 0.8074 - val_loss: 0.4520 - val_categorical_accuracy: 0.8398\n",
      "Epoch 10/20\n",
      "4/4 [==============================] - 0s 77ms/step - loss: 0.4612 - categorical_accuracy: 0.8183 - val_loss: 0.4868 - val_categorical_accuracy: 0.8268\n",
      "Epoch 11/20\n",
      "4/4 [==============================] - 0s 72ms/step - loss: 0.4554 - categorical_accuracy: 0.8306 - val_loss: 0.4367 - val_categorical_accuracy: 0.8355\n",
      "Epoch 12/20\n",
      "4/4 [==============================] - 0s 69ms/step - loss: 0.4502 - categorical_accuracy: 0.8360 - val_loss: 0.4531 - val_categorical_accuracy: 0.8615\n",
      "Epoch 13/20\n",
      "4/4 [==============================] - 0s 72ms/step - loss: 0.4274 - categorical_accuracy: 0.8476 - val_loss: 0.4167 - val_categorical_accuracy: 0.8528\n",
      "Epoch 14/20\n",
      "4/4 [==============================] - 0s 71ms/step - loss: 0.4077 - categorical_accuracy: 0.8445 - val_loss: 0.4013 - val_categorical_accuracy: 0.8701\n",
      "Epoch 15/20\n",
      "4/4 [==============================] - 0s 70ms/step - loss: 0.4079 - categorical_accuracy: 0.8461 - val_loss: 0.4367 - val_categorical_accuracy: 0.8442\n",
      "Epoch 16/20\n",
      "4/4 [==============================] - 0s 73ms/step - loss: 0.3854 - categorical_accuracy: 0.8507 - val_loss: 0.3644 - val_categorical_accuracy: 0.8831\n",
      "Epoch 17/20\n",
      "4/4 [==============================] - 0s 79ms/step - loss: 0.3777 - categorical_accuracy: 0.8538 - val_loss: 0.4085 - val_categorical_accuracy: 0.8398\n",
      "Epoch 18/20\n",
      "4/4 [==============================] - 0s 69ms/step - loss: 0.3642 - categorical_accuracy: 0.8577 - val_loss: 0.3628 - val_categorical_accuracy: 0.8788\n",
      "Epoch 19/20\n",
      "4/4 [==============================] - 0s 68ms/step - loss: 0.3499 - categorical_accuracy: 0.8623 - val_loss: 0.3959 - val_categorical_accuracy: 0.8528\n",
      "Epoch 20/20\n",
      "4/4 [==============================] - 0s 72ms/step - loss: 0.3347 - categorical_accuracy: 0.8677 - val_loss: 0.3765 - val_categorical_accuracy: 0.8701\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x24e8b482d50>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=20, batch_size=384, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 - 0s - loss: 0.3765 - categorical_accuracy: 0.8701 - 73ms/epoch - 9ms/step\n",
      "8/8 [==============================] - 0s 6ms/step\n",
      "tf.Tensor(\n",
      "[[46  2 12]\n",
      " [ 2 64  9]\n",
      " [ 4  1 91]], shape=(3, 3), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test, verbose=2)\n",
    "pred_test = model.predict(x_test)\n",
    "print(tf.math.confusion_matrix(y_test.argmax(axis=1), pred_test.argmax(axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Evaluate model on small dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 - 0s - loss: 0.3765 - categorical_accuracy: 0.8701 - 74ms/epoch - 9ms/step\n",
      "8/8 [==============================] - 0s 6ms/step\n",
      "tf.Tensor(\n",
      "[[46  2 12]\n",
      " [ 2 64  9]\n",
      " [ 4  1 91]], shape=(3, 3), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "model.evaluate(x_test_250, y_test_250, verbose=2)\n",
    "pred_test_250 = model.predict(x_test_250)\n",
    "print(tf.math.confusion_matrix(y_test_250.argmax(axis=1), pred_test_250.argmax(axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\luvluvdt3\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "model.save('lab_gsc.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove SoftMax layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if isinstance(model.layers[-1], Activation) and model.layers[-1].activation == softmax:\n",
    "    model = tf.keras.Model(model.input, model.layers[-2].output, name=model.name)\n",
    "else:\n",
    "    print('Error: last layer is not SoftMax Activation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Qualia-CodeGen for C inference code generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "Cannot find PyTorch, PyTorch framework will be unavailable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: qualia_codegen_core in c:\\users\\luvluvdt3\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.2.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\luvluvdt3\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from qualia_codegen_core) (1.26.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\luvluvdt3\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from qualia_codegen_core) (3.1.2)\n",
      "Requirement already satisfied: typing_extensions in c:\\users\\luvluvdt3\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from qualia_codegen_core) (4.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\luvluvdt3\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->qualia_codegen_core) (2.1.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install qualia_codegen_core\n",
    "import qualia_codegen_core\n",
    "from qualia_codegen_core.graph.KerasModelGraph import KerasModelGraph\n",
    "from qualia_codegen_core.graph.Quantization import Quantization\n",
    "from qualia_codegen_core.graph.RoundMode import RoundMode\n",
    "\n",
    "from importlib.resources import files\n",
    "main_path = str((files('qualia_codegen_core.examples')/'Linux'/'main.cpp').resolve())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Keras Model to Qualia-CodeGen's internal representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "—————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "Inputs                                           | Layer                                            | Outputs                                          | Input shape                                      | Output shape                                    \n",
      "—————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "                                                 | input_1                                          | conv1d                                           | (1, 16000, 1)                                    | ((1, 16000, 1),)                                \n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "input_1                                          | conv1d                                           | max_pooling1d                                    | (1, 16000, 1)                                    | ((1, 1599, 8),)                                 \n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "conv1d                                           | max_pooling1d                                    | conv1d_1                                         | (1, 1599, 8)                                     | ((1, 799, 8),)                                  \n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "max_pooling1d                                    | conv1d_1                                         | max_pooling1d_1                                  | (1, 799, 8)                                      | ((1, 198, 16),)                                 \n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "conv1d_1                                         | max_pooling1d_1                                  | conv1d_2                                         | (1, 198, 16)                                     | ((1, 99, 16),)                                  \n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "max_pooling1d_1                                  | conv1d_2                                         | max_pooling1d_2                                  | (1, 99, 16)                                      | ((1, 48, 32),)                                  \n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "conv1d_2                                         | max_pooling1d_2                                  | conv1d_3                                         | (1, 48, 32)                                      | ((1, 24, 32),)                                  \n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "max_pooling1d_2                                  | conv1d_3                                         | average_pooling1d                                | (1, 24, 32)                                      | ((1, 23, 64),)                                  \n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "conv1d_3                                         | average_pooling1d                                | flatten                                          | (1, 23, 64)                                      | ((1, 5, 64),)                                   \n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "average_pooling1d                                | flatten                                          | dense                                            | (1, 5, 64)                                       | ((1, 320),)                                     \n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "flatten                                          | dense                                            |                                                  | (1, 320)                                         | ((1, 3),)                                       \n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "modelgraph = KerasModelGraph(model).convert()\n",
    "print(modelgraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate C code for the trained model with 32-bit floating-point representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Graphviz not available\n"
     ]
    }
   ],
   "source": [
    "float_modelgraph = copy.deepcopy(modelgraph)\n",
    "\n",
    "# layer quantization annotations for float32\n",
    "for node in float_modelgraph.nodes:\n",
    "    # No scale factor if not fixed-point quantization on integers\n",
    "    node.q = Quantization(\n",
    "            number_type=float,\n",
    "            width=32,\n",
    "            long_width=32,\n",
    "            weights_scale_factor=0,\n",
    "            output_scale_factor=0,\n",
    "            weights_round_mode=RoundMode.NONE,\n",
    "            output_round_mode=RoundMode.NONE,\n",
    "            )\n",
    "\n",
    "float_res = qualia_codegen_core.Converter(output_path=Path('gsc_output_floating')).convert_model(float_modelgraph)\n",
    "\n",
    "with open('gsc_model_floating.h', 'w') as f:\n",
    "    f.write(float_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile the 32-bit floating-point C code for x86 and evaluate on small dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'g++' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'.' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!g++ -std=c++17 -Wall -Wextra -pedantic -Ofast -o gsc_floating -include gsc_output_floating/include/defines.h -Igsc_output_floating/include gsc_output_floating/model.c {main_path}\n",
    "!./gsc_floating x_test_gsc_250.csv y_test_gsc_250.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Generate C code for the trained model with 16-bit fixed-point representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Graphviz not available\n"
     ]
    }
   ],
   "source": [
    "fixed_modelgraph = copy.deepcopy(modelgraph)\n",
    "\n",
    "# layer quantization annotations for int16 Q9.7\n",
    "for node in fixed_modelgraph.nodes:\n",
    "    node.q = Quantization(\n",
    "            number_type=int,\n",
    "            width=16,\n",
    "            long_width=32,\n",
    "            weights_scale_factor=7,\n",
    "            output_scale_factor=7,\n",
    "            weights_round_mode=RoundMode.FLOOR,\n",
    "            output_round_mode=RoundMode.FLOOR,\n",
    "            )\n",
    "\n",
    "fixed_res = qualia_codegen_core.Converter(output_path=Path('gsc_output_fixed')).convert_model(fixed_modelgraph)\n",
    "\n",
    "with open('gsc_model_fixed.h', 'w') as f:\n",
    "    f.write(fixed_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Compile the 16-bit fixed-point C code for x86 and evaluate on small dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'g++' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'.' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!g++ -std=c++17 -Wall -Wextra -pedantic -Ofast -o gsc_fixed -include gsc_output_fixed/include/defines.h -Igsc_output_fixed/include gsc_output_fixed/model.c {main_path}\n",
    "!./gsc_fixed x_test_gsc_250.csv y_test_gsc_250.csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
